\begin{spacing}{1.5}
\phantom
\phantom
\phantom
\phantom
\section{Discussion}
\phantom
\phantom
Based on Cumming's (2005:773) definition of effectiveness (i.e., that the observed proportion of large fires in an area with aggressive suppression is lower than in areas without), it can now be said that the fire suppression strategy employed by the OMNR has indeed been effective over the study period 1989--2004. This study vindicates those fire managers at the Ministry of Natural Resources, who argued that fire suppression significantly reduces the annual area burned by large wildfires. Proving that the OMNR's fire management strategy has delivered in its objectives will help protect those services against calls to reallocate both funding and resources. \\

\noindent The new data on fire size distributions also helps fire managers plan  new strategies over the short-- to medium--term. The data can be used to develop estimates of how the forests in other fire management zones would react to an increase in fire suppression activities, thereby helping fire managers make more informed decisions about the delineation of fire management zones. Measuring the impact of aggressive fire suppression (i.e., the total area saved from forest fires due to fire management) could also help quantify the value of those resources protected by existing fire management strategies, details which are highly pertinent to the ongoing commercial timber operations in Ontario. Calculating this impact, however, requires the use of counterfactual modelling, and was, therefore, beyond the scope of this study.

\subsection{Epistemological Limitations}
While the results presented in this paper support the OMNR's strategy, there are still concerns over the long term sustainability of forest management in Ontario. These concerns arise from the epistemological limitations of the methods used in fire size distribution research itself. \\

\clearpage

\noindent The dynamics of forests and forest fire are complex and poorly understood phenomena (see \emph{Chapter 1.2}). The resultant fire size distributions are determined by many causal factors. These factors may interact with fire suppression, to either enhance or neutralise its effectiveness (Cui and Perera 2008:241). For example, while in this study, each forest fire was assumed to be an independent event (see \emph{Chapter 5.6.1}), we know that this in not true in reality. Forest fires can change the distribution of subsequent fires, which in turn, can determine the composition of future forests and their susceptibility to wildfire. Even research based on computer modelling of fire size distributions is not immune from oversimplification (Song \emph{et al}. 2001). Long term factors, such as climate patterns, pest epidemics and demographic changes could all significantly challenge the continued effectiveness of fire suppression (Cui and Perera 2008:238). Overcoming these limitations would require further studies, using GIS to explore the relationships between successive fires, on a much finer spatial scale than what could be achieved here. \\

\noindent Holling and Meffe (1996) have warned against overlooking the complexities of forest management. They argue that the aggressive fire suppression strategies, designed to increase the short--term predictability of forest ecosystems, may be leading to long--term instability, by reducing the forest's natural reliance to disturbances. In Yellowstone national park in the United States for example, fire suppression efforts were at first very successful. However, the consequence of those actions only became apparent years later, when the accumulation of unburned fuel eventually lead to ``\emph{fires of an intensity, extent, and human cost never before encountered}'' (Kilgore 1976; Christensen \emph{et al}. 1989). In the clearing created by the first large fire, huge swathes of new forest grew together, laying the foundations of the next fire, creating what was described as an ``\emph{unending cycle of monster fires and blackened landscapes}'' (Bonnicksen 2002:2). Holling and Meffe (1996:330) believe that the conflagrations seen in Yellowstone are an inevitable consequence of the same approach to forest fire management taken by the OMNR in Ontario. \\

\noindent The proposed solution to these concerns (which is also supported by Donovan and Brown 2007) requires forest managers to accept the natural range of variation within the forest ecosystem, rather than trying to change or control them (Holling and Meffe 1996:334). With the aim to restore forest ecosystems so that they are more representative of the prehistoric landscape (Bonnicksen 2002:5). These policies would require the OMNR to abandon large swathes of the provence to wilderness and halt further development. Considering the forests commercial significance, however, these ideas are highly unlikely ever to be implemented.

\subsection{Statistical Limitations}
While statistical analysis makes scientific studies possible, the methods contains within are not without their problems. As Nassim Nicolas Taleb (2008) states:

\begin{quote}
``\emph{Statistics is the core of knowledge; the logic of science; the tools of epistemology. However, statistics can fool you.}''
\end{quote}

\noindent Most of these problems originate from the necessary abstractions and assumptions made in the research design (see \emph{Chapter 3}). However, there are other, deeper issues, originating from studying the philosophy of science. While none prove fatal to the evidence presented here, for the sake of completeness and a more open science, they are worthy attention.

\subsubsection{Probabilistic Modus Tollens fallacy}
Hypothesis testing (as used in \emph{Chapter 4}) rests on a logical argument known as modus tollens (also known as 'denying the consequent') (Gill 1999:7). The logic can be explained as follows: \\

\begin{tabular}{ll}
If $P$ implies $Q$, and $Q$ is found to be false;\\
Then one must concluded that $P$ must also be false.\\
\end{tabular}\\

\noindent In scientific research, this method involves verifying a theoretical assumption with observations of real--world events. For example, if the null hypothesis $H_{\mathrm{0}}$ is true, then the data should follow an expected pattern (usually $\theta$ = 0). If the data does not follow this pattern therefore, one must conclude that $H_{\mathrm{0}}$ must be false. \\

\noindent However, when we replace certainties with probabilities (as is the case with the method employed in this study), the logic of modus tollens no longer holds true (Gill 1999:7). For example, it would be a fallacy to assert that; if the null hypothesis $H_{\mathrm{0}}$ is true, then the data is \emph{likely} to follow an expected pattern. And if the data does not follow this pattern, one must concluded that $H_{\mathrm{0}}$ is therefore \emph{unlikely}. Simply obtaining a sample of data that is atypical (such as that found in this study), in no way proves $H_{\mathrm{0}}$ to be false. Merely that, over the given study period, $H_{\mathrm{0}}$ has not yet been observed.\\

\noindent To put this in more concrete terms; while it can be said that over the study period 1989--2004, the aggressive fire suppression strategy has been effective at reducing the number of large forest fires, we cannot say with any certainty that this effect will continue indefinitely.

\subsubsection{Predicting Small Probability Events}
If we did wish to know whether the effect of fire suppression will continue in the future, more observations would be needed. However, given that the probabilities involved with large forest fires are very small, the number of observations needed grows exponentially. For example, to predict 1 year ahead, at least 10 or more years of data would be required. However, to predict 10 years ahead, more than 1000 years of data would be required to give us any remotely acceptable accuracy (Makridakis and Taleb 2009:2) In addition, estimates derived from these observations tend to come with greater error than for more frequent events. To solve this problem statisticians assume that these events follow a probability distribution (For example, in this study a logistic distribution was assumed, see \emph{Chapter 3}). This is done so that we can extrapolate that the pattern observed at the centre, out towards the extreme ends or 'tails' of the distribution. However, these distributions are often chosen in a rather tautological fashion. On one hand, we need the data to show us which probability distribution will allow us to extrapolate our existing knowledge, however, we also need to choose a probability distribution, \emph{a priori}, in order to gauge weather the data conforms to that particular distribution (Taleb 2007:2). These problems  presents somewhat of a philosophical quandary, from which there is no obvious solution.

\end{spacing}
\clearpage
